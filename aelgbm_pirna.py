# -*- coding: utf-8 -*-
"""AELGBM_piRNA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TdJFv6dBpCd92lo6C7BY1_J4gBtiEy2v
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
import scipy.io
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as pyplot
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc, precision_recall_curve
from keras.models import Sequential, Model
from keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Dense, Input
from keras.utils import to_categorical
from random import randint
from sklearn.preprocessing import LabelEncoder
from numpy import interp
from tensorflow.keras import regularizers
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, BatchNormalization
from sklearn.metrics import accuracy_score
from google.colab import drive
drive.mount('/content/drive')



def prepare_data(seperate=False):
    print ("loading data")




    disease_fea = pd.read_csv("/content/drive/My Drive/Colab Notebooks/d2d_do.csv", header=0, index_col=0).to_numpy()
    miRNA_feat = pd.read_csv("/content/drive/My Drive/Colab Notebooks/p2p_smith.csv", header=0, index_col=0).to_numpy()
    interactions = pd.read_csv("/content/drive/My Drive/Colab Notebooks/adj.csv", header=0, index_col=0).to_numpy().T

    miRNA_fea = miRNA_feat[:2174, :2174]
    interaction = interactions[:, :2174]

    disease_fea.shape
    miRNA_fea.shape
    interaction.shape
    print("interaction",interaction.shape)
    print("miRNA_fea",miRNA_fea.shape)
    print("disease_fea",disease_fea.shape)


    link_number = 0

    train = []
    testfnl= []
    label1 = []
    label2 = []
    label22=[]
    ttfnl=[]

    for i in range(interaction.shape[0]):   # shape[0] returns m if interaction is m*n, ie, returns no. of rows of matrix
        for j in range(interaction.shape[1]):

            if interaction[i, j] == 1:                   #for associated
                label1.append(interaction[i,j])             #label1= labels for association(1)
                link_number = link_number + 1               #no. of associated samples

                disease_fea_tmp = list(disease_fea[i])
                miRNA_fea_tmp = list(miRNA_fea[j])
                tmp_fea = miRNA_fea_tmp + disease_fea_tmp #concatnated feature vector for an association
                train.append(tmp_fea)                       #train contains feature vectors of all associated samples

            elif interaction[i,j] == 0:                     #for no association
                label2.append(interaction[i,j])             #label2= labels for no association(0)
                disease_fea_tmp1 = list(disease_fea[i])
                miRNA_fea_tmp1 = list(miRNA_fea[j])
                test_fea = miRNA_fea_tmp1 + disease_fea_tmp1 #concatenated feature vector for not having association
                testfnl.append(test_fea)                    #testfnl contains feature vectors of all non associated samples

    print(miRNA_fea[0])
    print(disease_fea[0])
    print(type(train))
    print("Data loaded")
    print("train shape", np.array(train).shape)
    m = np.arange(len(label2))
    print("Number",m)
    np.random.shuffle(m)

    for x in m:
        ttfnl.append(testfnl[x])
        label22.append(label2[x])

    print("Link number",link_number)
    for x in range(0, link_number):        #original link_number instead of 223          #for equalizing positive and negative samples
        tfnl= ttfnl[x]                                    #tfnl= feature vector pair for no association
        lab= label22[x]                                      #lab= label of the above mentioned feature vector pair(0)

        train.append(tfnl)                                  #append the non associated feature vector pairs to train till x<=no. of associated pairs
        label1.append(lab)
    print("train shape", np.array(train).shape)
    return np.array(train), label1, np.array(testfnl)

def calculate_performace(test_num, pred_y,  labels):
    tp =0
    fp = 0
    tn = 0
    fn = 0
    for index in range(test_num):
        if labels[index] ==1:
            if labels[index] == pred_y[index]:
                tp = tp +1
            else:
                fn = fn + 1
        else:
            if labels[index] == pred_y[index]:
                tn = tn +1
            else:
                fp = fp + 1

    acc = float(tp + tn)/test_num

    if tp == 0 and fp == 0:
        precision = 0
        MCC = 0
        f1_score=0
        sensitivity =  float(tp)/ (tp+fn)
        specificity = float(tn)/(tn + fp)
    else:
        precision = float(tp)/(tp+ fp)
        sensitivity = float(tp)/ (tp+fn)
        specificity = float(tn)/(tn + fp)
        MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))
        f1_score= float(2*tp)/((2*tp)+fp+fn)

    return acc, precision, sensitivity, specificity, MCC,f1_score

def transfer_array_format(data):
    formated_matrix1 = data[:, :219]  # Assuming the first 253 columns are for miRNA features
    formated_matrix2 = data[:, 219:]  # Assuming the remaining columns are for disease features

    return formated_matrix1, formated_matrix2

def preprocess_labels(labels, encoder=None, categorical=True):
    if not encoder:
        encoder = LabelEncoder()
        encoder.fit(labels)
    y = encoder.transform(labels).astype(np.int32)
    if categorical:
        y = to_categorical(y)
    return y, encoder

def DNN():
    model = nn.Sequential(
        nn.Linear(in_features=128, out_features=500),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(in_features=500, out_features=500),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(in_features=500, out_features=300),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(in_features=300, out_features=2),
        nn.Sigmoid()
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.95, eps=1e-08)
    return model, criterion, optimizer



def DNN_auto(x_train):

    encoding_dim = 128
    input_img = Input(shape=(2195,))


    encoded = Dense(350, activation='relu')(input_img)   # 450 - output (input layer)
    encoded = Dense(250, activation='relu')(encoded)     # 200 - output (hidden layer1)
    encoded = Dense(100, activation='relu')(encoded)     # 100 - output (hidden layer2)
    # Latent space with sparsity constraint
    encoder_output = Dense(encoding_dim, activity_regularizer=regularizers.l1(1e-5))(encoded)  # Encoding layer with L1 regularization
    print()

    decoded = Dense(100, activation='relu')(encoder_output)
    decoded = Dense(250, activation='relu')(decoded)
    decoded = Dense(350, activation='relu')(decoded)
    decoded = Dense(2195, activation='sigmoid')(decoded)

    autoencoder = Model(inputs=input_img, outputs=decoded)

    encoder = Model(inputs=input_img, outputs=encoder_output)


    autoencoder.compile(optimizer='adam', loss='mse')


    autoencoder.fit(x_train, x_train,epochs=20,batch_size=100,shuffle=True)  # second x_train is given instead of train labels in DNN, ie here, i/p=o/p


    encoded_imgs = encoder.predict(x_train)


    return encoder_output,encoded_imgs


def DeepMDA():
    X, labels,T = prepare_data(seperate = True)     #X= array of concatinated features,labels=corresponding labels

    X_data1, X_data2 = transfer_array_format(X)  # X-data1 = miRNA features(2500*495),  X_data2 = disease features (2500*383)


    print("************")
    print (X_data1.shape,X_data2.shape)  # (36352,512), (36352,71)
    print("******************")


    X_data1= np.concatenate((X_data1, X_data2 ), axis = 1)

    print("************")
    print (X_data1.shape)  # (36352,583)
    print("******************")


    y, encoder = preprocess_labels(labels)# labels labels_new
    num = np.arange(len(y))   #num gets an array like num = [0,1,2...len(y)], len(y) = 512*71 = 36352
    np.random.shuffle(num)
    X_data1 = X_data1[num]
    #X_data2 = X_data2[num]
    y = y[num]



    t=0
    mean_tpr = 0.0
    mean_fpr = np.linspace(0, 1, 100)





    num_cross_val = 5
    #all_performance = []
    #all_performance_rf = []
    #all_performance_bef = []
    all_performance_DNN = []
    #all_performance_SDADNN = []
    #all_performance_blend = []
    #all_labels = []
    #all_prob = {}
    #num_classifier = 5
    #all_prob[0] = []
    #all_prob[1] = []
    #all_prob[2] = []
    #all_prob[3] = []
    #all_averrage = []
    for fold in range(num_cross_val):
        train1 = np.array([x for i, x in enumerate(X_data1) if i % num_cross_val != fold])
        test1 = np.array([x for i, x in enumerate(X_data1) if i % num_cross_val == fold])
        train2 = np.array([x for i, x in enumerate(X_data2) if i % num_cross_val != fold])
        test2 = np.array([x for i, x in enumerate(X_data2) if i % num_cross_val == fold])
        train_label = np.array([x for i, x in enumerate(y) if i % num_cross_val != fold])
        test_label = np.array([x for i, x in enumerate(y) if i % num_cross_val == fold])
        #print("$$$$$$$$$$$$",test1)
        #print(test2)

        real_labels = []
        for val in test_label:
            if val[0] == 1:             #tuples in array, val[0]- first element of tuple
                real_labels.append(0)
            else:
                real_labels.append(1)

        train_label_new = []
        for val in train_label:
            if val[0] == 1:
                train_label_new.append(0)
            else:
                train_label_new.append(1)
        class_index = 0


        # Autoencoder-based feature extraction
        prefilter_train, prefilter_test = DNN_auto(train1), DNN_auto(test1)

        #prefilter_train_bef, prefilter_test_bef = autoencoder_two_subnetwork_fine_tuning(train1, train2, train_label, test1, test2, test_label)



        ## DNN
        class_index = class_index + 1
        prefilter_train = np.concatenate((train1, train2), axis = 1)
        prefilter_test = np.concatenate((test1, test2), axis = 1)

        prefilter_train = train1
        prefilter_test = test1

        model_DNN = DNN()

        encoder,encoder_imgs=DNN_auto(prefilter_train)



    #LightGBM Classifier
        class_index = class_index + 1
        prefilter_train = train1
        prefilter_test = test1

        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9
        }

        train_data = lgb.Dataset(prefilter_train, label=train_label_new)
        test_data = lgb.Dataset(prefilter_test, label=real_labels, reference=train_data)

        num_round = 100
        bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])

        lgb_y_pred_prob = bst.predict(prefilter_test)
        proba = transfer_label_from_prob(lgb_y_pred_prob)
        print(X_data1.shape)


    # Calculate performance metrics
        acc, precision, sensitivity, specificity, MCC, f1_score = calculate_performace(len(real_labels), proba,  real_labels)

    # ROC curve
        fpr, tpr, auc_thresholds = roc_curve(real_labels, lgb_y_pred_prob)
        auc_score = auc(fpr, tpr)
        scipy.io.savemat('raw_DNN',{'fpr':fpr,'tpr':tpr,'auc_score':auc_score})

    # AUPR score
        precision1, recall, pr_threshoLds = precision_recall_curve(real_labels, lgb_y_pred_prob)
        aupr_score = auc(recall, precision1)

        print("RAW_DNN:", acc, precision, sensitivity, specificity, MCC, auc_score, aupr_score, f1_score)
        all_performance_DNN.append([acc, precision, sensitivity, specificity, MCC, auc_score, aupr_score, f1_score])
        t = t + 1  # AUC fold number

        pyplot.plot(fpr,tpr,label= 'ROC fold %d (AUC = %0.4f)' % (t, auc_score))
        mean_tpr += interp(mean_fpr, fpr, tpr) # one dimensional interpolation
        mean_tpr[0] = 0.0


        pyplot.xlabel('False positive rate, (1-Specificity)')
        pyplot.ylabel('True positive rate,(Sensitivity)')
        pyplot.title('Receiver Operating Characteristic curve: 5-Fold CV')
        pyplot.legend()

    mean_tpr /= num_cross_val
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)



    print('******* LightGBM Classifier *****')
    print ('mean performance of LGB using raw feature')
    print (np.mean(np.array(all_performance_DNN), axis=0))
    Mean_Result=[]
    Mean_Result= np.mean(np.array(all_performance_DNN), axis=0)
    print ('---' * 20)
    print('Mean-Accuracy=', Mean_Result[0], '\n Mean-precision=', Mean_Result[1])
    print('Mean-Sensitivity=', Mean_Result[2], '\n Mean-Specificity=', Mean_Result[3])
    print('Mean-MCC=', Mean_Result[4], '\n' 'Mean-auc_score=', Mean_Result[5])
    print('Mean-Aupr-score=', Mean_Result[6], '\n' 'Mean_F1=', Mean_Result[7])
    print('---' * 20)



    print(X_data1.shape)

    #Plot ROC Curve
    pyplot.plot(mean_fpr, mean_tpr,'--' ,linewidth=3.5,label='Mean ROC (AUC = %0.4f)' % Mean_Result[5])
    pyplot.legend()


    pyplot.show()




def transfer_label_from_prob(proba):
    label = [1 if val>=0.7 else 0 for val in proba]
    return label


if __name__=="__main__":
    DeepMDA()